# last Neursl NETWORK code with 86.73% maximum

# ============================================
# ENHANCED NEURAL NETWORK WITH ALL IMPROVEMENTS
# ============================================

# Step 1: Import Libraries
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

# Set random seeds for reproducibility
torch.manual_seed(42)
np.random.seed(42)

print("=" * 60)
print("ENHANCED NEURAL NETWORK WITH ALL IMPROVEMENTS")
print("=" * 60)
print()

# Step 2: Load ORIGINAL Data and Apply Feature Engineering
print("Loading original data for feature engineering...")
try:
    # Load original dataset with YOUR Kaggle path
    df = pd.read_csv('/kaggle/input/insurance/insurance.csv')
    print(f"‚úÖ Original data loaded: {df.shape[0]} rows, {df.shape[1]} columns")
except FileNotFoundError:
    print("‚ùå insurance.csv not found at /kaggle/input/insurance/insurance.csv!")
    print("Trying alternative path...")
    try:
        # Try alternative path
        df = pd.read_csv('insurance.csv')
        print(f"‚úÖ Original data loaded from alternative path")
    except:
        print("‚ùå Could not find insurance.csv. Please check file path.")
        raise

# ============================================
# FEATURE ENGINEERING - CRITICAL IMPROVEMENT
# ============================================
print("\n" + "=" * 60)
print("APPLYING ADVANCED FEATURE ENGINEERING")
print("=" * 60)

# Create new features BEFORE preprocessing
print("Creating new features...")

# 1. Polynomial features
df['age_squared'] = df['age'] ** 2
df['bmi_squared'] = df['bmi'] ** 2

# 2. Interaction features
df['age_times_bmi'] = df['age'] * df['bmi'] / 100

# 3. Smoker interactions (MOST IMPORTANT!)
df['smoker_age'] = df['age'] * (df['smoker'] == 'yes').astype(int)
df['smoker_bmi'] = df['bmi'] * (df['smoker'] == 'yes').astype(int)

# 4. Risk score calculation
df['risk_score'] = df['age']/10 + df['bmi']/5 + df['children']*2

# 5. BMI categories
df['is_obese'] = (df['bmi'] >= 30).astype(int)
df['is_overweight'] = ((df['bmi'] >= 25) & (df['bmi'] < 30)).astype(int)

# 6. Age groups (simplified for now)
df['age_group'] = pd.cut(df['age'], 
                        bins=[18, 30, 40, 50, 60, 65],
                        labels=['18-29', '30-39', '40-49', '50-59', '60+'])

# 7. Children interaction
df['children_age'] = df['children'] * df['age'] / 10

print("‚úÖ New features created:")
print(f"   Total features now: {df.shape[1] - 1} (excluding charges)")
print(f"   Original features: 6")
print(f"   New features: {df.shape[1] - 1 - 6}")
print()

# Step 3: Split into X and y (with new features)
print("Separating features and target...")
# Keep original features + new engineered features
X = df.drop(columns=['charges'])
y = df['charges']

print(f"X shape after feature engineering: {X.shape}")
print(f"y shape: {y.shape}")
print()

# Step 4: Train/Validation/Test Split (60/20/20)
print("=" * 60)
print("DATASET SPLITTING (60/20/20)")
print("=" * 60)

# First split: 60% train, 40% temp (val+test)
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.4, random_state=42
)

# Second split: 50% of temp = validation, 50% = test
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.5, random_state=42
)

print("Split Results:")
print(f"Training set:   {X_train.shape[0]} samples ({X_train.shape[0]/len(df)*100:.1f}%)")
print(f"Validation set: {X_val.shape[0]} samples ({X_val.shape[0]/len(df)*100:.1f}%)")
print(f"Test set:       {X_test.shape[0]} samples ({X_test.shape[0]/len(df)*100:.1f}%)")
print()

# Step 5: Preprocessing with NEW features
print("=" * 60)
print("PREPROCESSING WITH ENHANCED FEATURES")
print("=" * 60)

# Identify column types for new dataset
# Numerical columns: all continuous values
numerical_cols = ['age', 'bmi', 'children', 'age_squared', 'bmi_squared', 
                  'age_times_bmi', 'smoker_age', 'smoker_bmi', 'risk_score',
                  'is_obese', 'is_overweight', 'children_age']

# Categorical columns: strings or categories
categorical_cols = ['sex', 'smoker', 'region', 'age_group']

print(f"Numerical columns ({len(numerical_cols)}):")
for i, col in enumerate(numerical_cols):
    print(f"  {i+1:2}. {col}")
print()

print(f"Categorical columns ({len(categorical_cols)}):")
for i, col in enumerate(categorical_cols):
    print(f"  {i+1:2}. {col}")
print()

# Create enhanced preprocessor
preprocessor = ColumnTransformer([
    ('num', StandardScaler(), numerical_cols),
    ('cat', OneHotEncoder(drop='first', sparse_output=False), categorical_cols)  # Changed sparse_output
])

# Fit and transform
print("Fitting preprocessor on training data...")
X_train_processed = preprocessor.fit_transform(X_train)
X_val_processed = preprocessor.transform(X_val)
X_test_processed = preprocessor.transform(X_test)

# Get feature names
cat_encoder = preprocessor.named_transformers_['cat']
cat_feature_names = cat_encoder.get_feature_names_out(categorical_cols)
all_feature_names = list(numerical_cols) + list(cat_feature_names)

print(f"\n‚úÖ Preprocessing completed!")
print(f"Original X_train shape: {X_train.shape}")
print(f"Processed X_train shape: {X_train_processed.shape}")
print(f"Total features after preprocessing: {len(all_feature_names)}")
print(f"First 5 feature names: {all_feature_names[:5]}")
print()

# Convert to numpy arrays (already dense due to sparse_output=False)
X_train_array = X_train_processed
X_val_array = X_val_processed
X_test_array = X_test_processed

# Step 6: SCALE THE TARGET VARIABLE
print("=" * 60)
print("SCALING TARGET VARIABLE (CHARGES)")
print("=" * 60)

y_scaler = StandardScaler()
y_train_scaled = y_scaler.fit_transform(y_train.values.reshape(-1, 1)).flatten()
y_val_scaled = y_scaler.transform(y_val.values.reshape(-1, 1)).flatten()
y_test_scaled = y_scaler.transform(y_test.values.reshape(-1, 1)).flatten()

print(f"Original y range: ${y_train.min():,.2f} to ${y_train.max():,.2f}")
print(f"Scaled y range: {y_train_scaled.min():.3f} to {y_train_scaled.max():.3f}")
print()

# Step 7: Convert to PyTorch Tensors
print("Converting to PyTorch tensors...")
X_train_tensor = torch.FloatTensor(X_train_array)
y_train_tensor = torch.FloatTensor(y_train_scaled).reshape(-1, 1)

X_val_tensor = torch.FloatTensor(X_val_array)
y_val_tensor = torch.FloatTensor(y_val_scaled).reshape(-1, 1)

X_test_tensor = torch.FloatTensor(X_test_array)
y_test_tensor = torch.FloatTensor(y_test_scaled).reshape(-1, 1)

print(f"‚úÖ Data converted to PyTorch tensors")
print(f"Input size: {X_train_tensor.shape[1]} features")
print(f"Training samples: {X_train_tensor.shape[0]}")
print(f"Validation samples: {X_val_tensor.shape[0]}")
print(f"Test samples: {X_test_tensor.shape[0]}")
print()

# Step 8: Create DataLoader
print("Creating DataLoaders...")
batch_size = 32  # Increased for better convergence

train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
val_dataset = TensorDataset(X_val_tensor, y_val_tensor)
test_dataset = TensorDataset(X_test_tensor, y_test_tensor)

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

print(f"‚úÖ DataLoaders created")
print(f"Batch size: {batch_size}")
print(f"Training batches: {len(train_loader)}")
print(f"Validation batches: {len(val_loader)}")
print(f"Test batches: {len(test_loader)}")
print()

# ============================================
# ENHANCED NEURAL NETWORK ARCHITECTURE
# ============================================
print("=" * 60)
print("ENHANCED NEURAL NETWORK ARCHITECTURE")
print("=" * 60)

class EnhancedInsuranceNet(nn.Module):
    def __init__(self, input_size):
        super(EnhancedInsuranceNet, self).__init__()
        
        # Main network
        self.layer1 = nn.Linear(input_size, 64)
        self.bn1 = nn.BatchNorm1d(64)
        self.drop1 = nn.Dropout(0.3)
        
        self.layer2 = nn.Linear(64, 32)
        self.bn2 = nn.BatchNorm1d(32)
        self.drop2 = nn.Dropout(0.2)
        
        self.layer3 = nn.Linear(32, 16)
        self.bn3 = nn.BatchNorm1d(16)
        
        self.output = nn.Linear(16, 1)
        
        # Initialize weights
        self._initialize_weights()
    
    def _initialize_weights(self):
        for layer in [self.layer1, self.layer2, self.layer3, self.output]:
            if isinstance(layer, nn.Linear):
                nn.init.kaiming_normal_(layer.weight, nonlinearity='relu')
                if layer.bias is not None:
                    nn.init.zeros_(layer.bias)
    
    def forward(self, x):
        out = self.layer1(x)
        out = self.bn1(out)
        out = torch.relu(out)
        out = self.drop1(out)
        
        out = self.layer2(out)
        out = self.bn2(out)
        out = torch.relu(out)
        out = self.drop2(out)
        
        out = self.layer3(out)
        out = self.bn3(out)
        out = torch.relu(out)
        
        out = self.output(out)
        
        return out

# Create model
input_size = X_train_tensor.shape[1]
model = EnhancedInsuranceNet(input_size)

print("Enhanced Model Architecture:")
print(model)
print()
print(f"Total parameters: {sum(p.numel() for p in model.parameters()):,}")
print(f"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")
print()

# ============================================
# ADVANCED TRAINING SETUP
# ============================================
print("=" * 60)
print("ADVANCED TRAINING SETUP")
print("=" * 60)

# Use Huber loss (less sensitive to outliers)
criterion = nn.HuberLoss(delta=1.0)
print(f"Loss function: HuberLoss (delta=1.0)")

# Optimizer with different settings
optimizer = optim.AdamW(model.parameters(), 
                       lr=0.0003,
                       weight_decay=1e-3,
                       betas=(0.9, 0.999))

# Learning rate scheduler
scheduler = optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, 
    mode='min',
    factor=0.5,
    patience=10,
    min_lr=1e-6,
    verbose=False
)

print(f"Optimizer: AdamW")
print(f"Initial learning rate: {optimizer.param_groups[0]['lr']:.6f}")
print(f"Weight decay: {optimizer.param_groups[0]['weight_decay']:.0e}")
print()

# ============================================
# TRAINING FUNCTIONS
# ============================================
def train_epoch_enhanced(model, loader, criterion, optimizer):
    """Train for one epoch with gradient clipping"""
    model.train()
    total_loss = 0
    
    for batch_x, batch_y in loader:
        optimizer.zero_grad()
        predictions = model(batch_x)
        loss = criterion(predictions, batch_y)
        loss.backward()
        
        # Gradient clipping
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        
        optimizer.step()
        total_loss += loss.item()
    
    return total_loss / len(loader)

def validate_enhanced(model, loader, criterion):
    """Validate the model"""
    model.eval()
    total_loss = 0
    all_predictions = []
    all_targets = []
    
    with torch.no_grad():
        for batch_x, batch_y in loader:
            predictions = model(batch_x)
            loss = criterion(predictions, batch_y)
            total_loss += loss.item()
            
            all_predictions.extend(predictions.cpu().numpy())
            all_targets.extend(batch_y.cpu().numpy())
    
    return total_loss / len(loader), np.array(all_predictions), np.array(all_targets)

# ============================================
# TRAINING LOOP
# ============================================
print("=" * 60)
print("TRAINING ENHANCED NEURAL NETWORK")
print("=" * 60)

num_epochs = 200  # Reduced for testing
patience = 30  # Reduced patience for testing
best_val_loss = float('inf')
patience_counter = 0

# Store history
train_losses = []
val_losses = []
learning_rates = []

print(f"Training for {num_epochs} epochs (patience={patience})...")
print("-" * 60)

for epoch in range(num_epochs):
    # Train
    train_loss = train_epoch_enhanced(model, train_loader, criterion, optimizer)
    train_losses.append(train_loss)
    
    # Validate
    val_loss, val_preds_scaled, val_targets_scaled = validate_enhanced(model, val_loader, criterion)
    val_losses.append(val_loss)
    
    # Store learning rate
    learning_rates.append(optimizer.param_groups[0]['lr'])
    
    # Update scheduler
    scheduler.step(val_loss)
    
    # Early stopping and model saving
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        patience_counter = 0
        
        # Save best model
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'val_loss': val_loss,
            'y_scaler_mean': y_scaler.mean_,
            'y_scaler_scale': y_scaler.scale_
        }, 'enhanced_best_nn_model.pth')
        
        if epoch < 10 or (epoch + 1) % 20 == 0:
            print(f"üíæ Saved best model at epoch {epoch+1} (val loss: {val_loss:.4f})")
    else:
        patience_counter += 1
    
    # Print progress
    if (epoch + 1) % 20 == 0:
        current_lr = optimizer.param_groups[0]['lr']
        
        # Convert some predictions back to original scale
        sample_preds_original = y_scaler.inverse_transform(
            val_preds_scaled[:3].reshape(-1, 1)).flatten()
        sample_targets_original = y_scaler.inverse_transform(
            val_targets_scaled[:3].reshape(-1, 1)).flatten()
        
        print(f"Epoch [{epoch+1:3d}/{num_epochs}] | "
              f"Train Loss: {train_loss:.4f} | "
              f"Val Loss: {val_loss:.4f} | "
              f"LR: {current_lr:.6f}")
        print(f"  Predictions: {[f'${p:,.0f}' for p in sample_preds_original]}")
        print(f"  Actual:      {[f'${t:,.0f}' for t in sample_targets_original]}")
        print(f"  Patience: {patience_counter}/{patience}")
        print("-" * 40)
    
    # Early stopping
    if patience_counter >= patience:
        print(f"\n‚úÖ Early stopping triggered at epoch {epoch+1}")
        break

print("-" * 60)
print(f"‚úÖ Training completed!")
print(f"Best validation loss: {best_val_loss:.4f}")
print(f"Final epoch: {epoch+1}")
print()

# ============================================
# EVALUATION
# ============================================
print("=" * 60)
print("MODEL EVALUATION")
print("=" * 60)

def inverse_transform_predictions(model, loader, y_scaler):
    """Get predictions and inverse transform to original scale"""
    model.eval()
    all_preds_scaled = []
    all_targets_scaled = []
    
    with torch.no_grad():
        for batch_x, batch_y in loader:
            preds = model(batch_x)
            all_preds_scaled.extend(preds.cpu().numpy())
            all_targets_scaled.extend(batch_y.cpu().numpy())
    
    # Convert to numpy arrays
    preds_scaled = np.array(all_preds_scaled).reshape(-1, 1)
    targets_scaled = np.array(all_targets_scaled).reshape(-1, 1)
    
    # Inverse transform to original scale
    preds_original = y_scaler.inverse_transform(preds_scaled).flatten()
    targets_original = y_scaler.inverse_transform(targets_scaled).flatten()
    
    return preds_original, targets_original

# Get predictions
print("Getting predictions in original scale...")
train_preds, train_targets = inverse_transform_predictions(model, train_loader, y_scaler)
val_preds, val_targets = inverse_transform_predictions(model, val_loader, y_scaler)
test_preds, test_targets = inverse_transform_predictions(model, test_loader, y_scaler)

print(f"‚úÖ Predictions obtained")
print()

# Calculate metrics
def calculate_all_metrics(predictions, targets):
    mae = mean_absolute_error(targets, predictions)
    rmse = np.sqrt(mean_squared_error(targets, predictions))
    r2 = r2_score(targets, predictions)
    mape = np.mean(np.abs((targets - predictions) / np.clip(targets, 1e-10, None))) * 100
    return mae, rmse, r2, mape

train_mae, train_rmse, train_r2, train_mape = calculate_all_metrics(train_preds, train_targets)
val_mae, val_rmse, val_r2, val_mape = calculate_all_metrics(val_preds, val_targets)
test_mae, test_rmse, test_r2, test_mape = calculate_all_metrics(test_preds, test_targets)

print("üìä PERFORMANCE METRICS:")
print("=" * 60)
print(f"{'Set':<15} {'MAE ($)':<12} {'RMSE ($)':<12} {'R¬≤ Score':<10} {'MAPE (%)':<10}")
print("-" * 60)
print(f"{'Training':<15} {train_mae:,.2f}{'':<2} {train_rmse:,.2f}{'':<2} {train_r2:.4f}{'':<2} {train_mape:.1f}%")
print(f"{'Validation':<15} {val_mae:,.2f}{'':<2} {val_rmse:,.2f}{'':<2} {val_r2:.4f}{'':<2} {val_mape:.1f}%")
print(f"{'Test':<15} {test_mae:,.2f}{'':<2} {test_rmse:,.2f}{'':<2} {test_r2:.4f}{'':<2} {test_mape:.1f}%")
print("=" * 60)
print()

# ============================================
# VISUALIZATION
# ============================================
print("Generating visualizations...")

fig, axes = plt.subplots(2, 3, figsize=(15, 10))

# Plot 1: Training history
axes[0, 0].plot(train_losses, label='Training Loss', linewidth=2, color='blue')
axes[0, 0].plot(val_losses, label='Validation Loss', linewidth=2, color='red')
axes[0, 0].set_xlabel('Epoch')
axes[0, 0].set_ylabel('Loss (Huber)')
axes[0, 0].set_title('Training History')
axes[0, 0].legend()
axes[0, 0].grid(True, alpha=0.3)

# Plot 2: Learning rate
axes[0, 1].plot(learning_rates, linewidth=2, color='green')
axes[0, 1].set_xlabel('Epoch')
axes[0, 1].set_ylabel('Learning Rate')
axes[0, 1].set_title('Learning Rate Schedule')
axes[0, 1].set_yscale('log')
axes[0, 1].grid(True, alpha=0.3)

# Plot 3: Training set predictions
axes[0, 2].scatter(train_targets, train_preds, alpha=0.5, color='blue', s=10)
max_val = max(train_targets.max(), train_preds.max())
axes[0, 2].plot([0, max_val], [0, max_val], 'r--', linewidth=2)
axes[0, 2].set_xlabel('Actual Charges ($)')
axes[0, 2].set_ylabel('Predicted Charges ($)')
axes[0, 2].set_title(f'Training Set\nR¬≤ = {train_r2:.4f}')
axes[0, 2].grid(True, alpha=0.3)

# Plot 4: Validation set predictions
axes[1, 0].scatter(val_targets, val_preds, alpha=0.5, color='orange', s=10)
max_val = max(val_targets.max(), val_preds.max())
axes[1, 0].plot([0, max_val], [0, max_val], 'r--', linewidth=2)
axes[1, 0].set_xlabel('Actual Charges ($)')
axes[1, 0].set_ylabel('Predicted Charges ($)')
axes[1, 0].set_title(f'Validation Set\nR¬≤ = {val_r2:.4f}')
axes[1, 0].grid(True, alpha=0.3)

# Plot 5: Test set predictions
axes[1, 1].scatter(test_targets, test_preds, alpha=0.5, color='green', s=10)
max_val = max(test_targets.max(), test_preds.max())
axes[1, 1].plot([0, max_val], [0, max_val], 'r--', linewidth=2)
axes[1, 1].set_xlabel('Actual Charges ($)')
axes[1, 1].set_ylabel('Predicted Charges ($)')
axes[1, 1].set_title(f'Test Set\nR¬≤ = {test_r2:.4f}')
axes[1, 1].grid(True, alpha=0.3)

# Plot 6: Performance summary
summary_text = f"""FINAL PERFORMANCE
Test Set:
‚Ä¢ R¬≤ Score: {test_r2:.4f}
‚Ä¢ MAE: ${test_mae:,.0f}
‚Ä¢ RMSE: ${test_rmse:,.0f}
‚Ä¢ MAPE: {test_mape:.1f}%

Improvements:
‚Ä¢ +{(test_r2 - 0.7957)*100:.1f}% over baseline
‚Ä¢ -${(2642.23 - test_mae):,.0f} better MAE"""

axes[1, 2].text(0.5, 0.5, summary_text, 
                ha='center', va='center', fontsize=12,
                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
axes[1, 2].axis('off')
axes[1, 2].set_title('Performance Summary')

plt.tight_layout()
plt.savefig('enhanced_nn_results.png', dpi=300, bbox_inches='tight')
plt.show()

# ============================================
# FINAL SUMMARY
# ============================================
print("=" * 60)
print("ENHANCED NEURAL NETWORK - FINAL SUMMARY")
print("=" * 60)

print("‚úÖ IMPROVEMENTS APPLIED:")
print("   1. Advanced Feature Engineering (+10 new features)")
print("   2. Enhanced NN Architecture (64‚Üí32‚Üí16‚Üí1 with BatchNorm)")
print("   3. Huber Loss Function (robust to outliers)")
print("   4. AdamW Optimizer with Weight Decay")
print("   5. Learning Rate Scheduling")
print("   6. Early Stopping (patience=30)")
print()

print("üìä FINAL PERFORMANCE (Test Set):")
print(f"   Mean Absolute Error: ${test_mae:,.2f}")
print(f"   Root Mean Squared Error: ${test_rmse:,.2f}")
print(f"   R¬≤ Score: {test_r2:.4f} ({test_r2*100:.2f}%)")
print(f"   Mean Absolute Percentage Error: {test_mape:.1f}%")
print()

# Compare with your previous best
previous_r2 = 0.8657
improvement = (test_r2 - previous_r2) * 100
relative_improvement = (test_r2 - previous_r2) / previous_r2 * 100

print("üìà COMPARISON WITH PREVIOUS BEST (R¬≤ = 0.8657):")
if test_r2 > previous_r2:
    print(f"   üéâ IMPROVED by +{improvement:.2f} percentage points")
    print(f"   üìä Relative improvement: {relative_improvement:.1f}%")
else:
    print(f"   ‚ö†Ô∏è  Similar performance: {test_r2:.4f} vs {previous_r2:.4f}")
    print(f"   This may be near the dataset's theoretical limit")
print()

print("üíæ FILES SAVED:")
print("   'enhanced_best_nn_model.pth' - Best model weights")
print("   'enhanced_nn_results.png' - Performance plots")
print()

print("üéØ NEXT STEPS:")
print("   1. Implement Decision Tree and Random Forest")
print("   2. Compare all 3 models")
print("   3. Write final report")
print()

print("=" * 60)
