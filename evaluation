# ============================================
# MODEL PERFORMANCE & EVALUATION
# ============================================

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

print("=" * 60)
print("MODEL PERFORMANCE & EVALUATION")
print("=" * 60)
print()

# ============================================
# 1. PERFORMANCE METRICS ON ALL SETS
# ============================================

print("üìä PERFORMANCE METRICS FOR ALL MODELS")
print("-" * 50)

# Define the metrics we collected from each model
# Neural Network (from previous output)
nn_metrics = {
    'Training': {'MAE': 2357.38, 'RMSE': 4514.37, 'R2': 0.8593},
    'Validation': {'MAE': 2333.20, 'RMSE': 4507.66, 'R2': 0.8558},
    'Test': {'MAE': 2367.77, 'RMSE': 4536.41, 'R2': 0.8673}
}

# Decision Tree (from previous output)
dt_metrics = {
    'Training': {'MAE': 2457.11, 'RMSE': 4241.61, 'R2': 0.8757},
    'Validation': {'MAE': 2564.16, 'RMSE': 4720.49, 'R2': 0.8418},
    'Test': {'MAE': 2850.36, 'RMSE': 4747.02, 'R2': 0.8547}
}

# Random Forest (from previous output)
rf_metrics = {
    'Training': {'MAE': 2015.00, 'RMSE': 4445.00, 'R2': 0.8597},
    'Validation': {'MAE': 2004.00, 'RMSE': 4433.00, 'R2': 0.8605},
    'Test': {'MAE': 2112.00, 'RMSE': 4551.00, 'R2': 0.8665}
}

# Create comparison DataFrame
comparison_data = []

for model_name, metrics in [('Neural Network', nn_metrics), 
                            ('Decision Tree', dt_metrics), 
                            ('Random Forest', rf_metrics)]:
    for dataset in ['Training', 'Validation', 'Test']:
        comparison_data.append({
            'Model': model_name,
            'Dataset': dataset,
            'MAE ($)': metrics[dataset]['MAE'],
            'RMSE ($)': metrics[dataset]['RMSE'],
            'R¬≤': metrics[dataset]['R2']
        })

comparison_df = pd.DataFrame(comparison_data)
pivot_df = comparison_df.pivot(index='Model', columns='Dataset')

print("Performance Comparison Table:")
print("=" * 75)

# Format and display the table
metrics_order = ['MAE ($)', 'RMSE ($)', 'R¬≤']
for metric in metrics_order:
    print(f"\n{metric}:")
    print("-" * 40)
    metric_df = pivot_df[metric]
    # Format numbers
    if metric in ['MAE ($)', 'RMSE ($)']:
        metric_df = metric_df.applymap(lambda x: f"${x:,.0f}")
    else:
        metric_df = metric_df.applymap(lambda x: f"{x:.4f}")
    
    print(metric_df.to_string())
    print()

# ============================================
# 2. OVERFITTING/UNDERFITTING ANALYSIS
# ============================================

print("üîç OVERFITTING/UNDERFITTING ANALYSIS")
print("-" * 50)

for model_name in ['Neural Network', 'Decision Tree', 'Random Forest']:
    model_data = comparison_df[comparison_df['Model'] == model_name]
    
    # Calculate performance gaps
    train_r2 = model_data[model_data['Dataset'] == 'Training']['R¬≤'].values[0]
    val_r2 = model_data[model_data['Dataset'] == 'Validation']['R¬≤'].values[0]
    test_r2 = model_data[model_data['Dataset'] == 'Test']['R¬≤'].values[0]
    
    gap_train_val = train_r2 - val_r2
    gap_val_test = abs(val_r2 - test_r2)
    
    print(f"\n{model_name}:")
    print(f"  Training R¬≤: {train_r2:.4f}")
    print(f"  Validation R¬≤: {val_r2:.4f}")
    print(f"  Test R¬≤: {test_r2:.4f}")
    print(f"  Train-Val Gap: {gap_train_val:.4f}")
    print(f"  Val-Test Gap: {gap_val_test:.4f}")
    
    # Classify overfitting/underfitting
    if gap_train_val > 0.05:
        print(f"  ‚ö†Ô∏è  OVERFITTING: Large gap between train and validation")
    elif gap_train_val < 0.01:
        print(f"  ‚úÖ GOOD GENERALIZATION: Small train-val gap")
    else:
        print(f"  ‚ö†Ô∏è  MODERATE OVERFITTING: Moderate train-val gap")
    
    if test_r2 > val_r2:
        print(f"  ‚úÖ GOOD: Test performance better than validation")
    elif abs(test_r2 - val_r2) < 0.02:
        print(f"  ‚úÖ STABLE: Similar validation and test performance")
    else:
        print(f"  ‚ö†Ô∏è  CAUTION: Test performance worse than validation")

# ============================================
# 3. VISUALIZATIONS
# ============================================

print("\nüìà GENERATING VISUALIZATIONS...")
print("-" * 50)

# Create figure with subplots
fig, axes = plt.subplots(2, 2, figsize=(15, 10))
fig.suptitle('Model Performance & Evaluation', fontsize=16, fontweight='bold')

# 1. R¬≤ Comparison Bar Chart
ax1 = axes[0, 0]
models = ['Neural Network', 'Decision Tree', 'Random Forest']
test_r2_values = [nn_metrics['Test']['R2'], dt_metrics['Test']['R2'], rf_metrics['Test']['R2']]

colors = ['#3498db', '#2ecc71', '#e74c3c']
bars = ax1.bar(models, test_r2_values, color=colors, edgecolor='black', linewidth=1.5)

ax1.set_ylabel('R¬≤ Score', fontweight='bold')
ax1.set_title('Test Set R¬≤ Comparison', fontweight='bold')
ax1.set_ylim([0.8, 0.9])
ax1.axhline(y=0.85, color='gray', linestyle='--', alpha=0.7, label='Good (0.85)')
ax1.axhline(y=0.87, color='green', linestyle='--', alpha=0.7, label='Excellent (0.87)')
ax1.grid(True, alpha=0.3, axis='y')

for bar, val in zip(bars, test_r2_values):
    height = bar.get_height()
    ax1.text(bar.get_x() + bar.get_width()/2, height + 0.002,
            f'{val:.4f}', ha='center', va='bottom', fontsize=10, fontweight='bold')

ax1.legend()

# 2. MAE Comparison
ax2 = axes[0, 1]
mae_values = [nn_metrics['Test']['MAE'], dt_metrics['Test']['MAE'], rf_metrics['Test']['MAE']]
bars2 = ax2.bar(models, mae_values, color=colors, edgecolor='black', linewidth=1.5)

ax2.set_ylabel('MAE ($)', fontweight='bold')
ax2.set_title('Test Set MAE Comparison (Lower is Better)', fontweight='bold')
ax2.grid(True, alpha=0.3, axis='y')

for bar, val in zip(bars2, mae_values):
    height = bar.get_height()
    ax2.text(bar.get_x() + bar.get_width()/2, height + 50,
            f'${val:,.0f}', ha='center', va='bottom', fontsize=10, fontweight='bold')

# 3. Overfitting Analysis (R¬≤ gaps)
ax3 = axes[1, 0]
gap_data = []
for model_name in models:
    if model_name == 'Neural Network':
        gap = nn_metrics['Training']['R2'] - nn_metrics['Validation']['R2']
    elif model_name == 'Decision Tree':
        gap = dt_metrics['Training']['R2'] - dt_metrics['Validation']['R2']
    else:
        gap = rf_metrics['Training']['R2'] - rf_metrics['Validation']['R2']
    gap_data.append(gap)

bars3 = ax3.bar(models, gap_data, color=colors, edgecolor='black', linewidth=1.5)
ax3.set_ylabel('R¬≤ Gap (Train - Val)', fontweight='bold')
ax3.set_title('Overfitting Analysis (Smaller gap = better)', fontweight='bold')
ax3.axhline(y=0, color='black', linewidth=1)
ax3.axhline(y=0.02, color='orange', linestyle='--', alpha=0.7, label='Acceptable (0.02)')
ax3.axhline(y=0.05, color='red', linestyle='--', alpha=0.7, label='High (0.05)')
ax3.grid(True, alpha=0.3, axis='y')

for bar, val in zip(bars3, gap_data):
    height = bar.get_height()
    ax3.text(bar.get_x() + bar.get_width()/2, height + 0.002 if height > 0 else height - 0.003,
            f'{val:.4f}', ha='center', va='bottom' if height > 0 else 'top', 
            fontsize=10, fontweight='bold')

ax3.legend()

# 4. Performance Summary Heatmap
ax4 = axes[1, 1]
# Prepare data for heatmap
heatmap_data = []
for model in models:
    if model == 'Neural Network':
        heatmap_data.append([nn_metrics['Training']['R2'], nn_metrics['Validation']['R2'], nn_metrics['Test']['R2']])
    elif model == 'Decision Tree':
        heatmap_data.append([dt_metrics['Training']['R2'], dt_metrics['Validation']['R2'], dt_metrics['Test']['R2']])
    else:
        heatmap_data.append([rf_metrics['Training']['R2'], rf_metrics['Validation']['R2'], rf_metrics['Test']['R2']])

heatmap_data = pd.DataFrame(heatmap_data, index=models, columns=['Train', 'Validation', 'Test'])
sns.heatmap(heatmap_data, annot=True, fmt='.4f', cmap='YlOrRd', 
            cbar_kws={'label': 'R¬≤ Score'}, ax=ax4, vmin=0.8, vmax=0.9)
ax4.set_title('R¬≤ Performance Heatmap', fontweight='bold')
ax4.set_xlabel('Dataset', fontweight='bold')
ax4.set_ylabel('Model', fontweight='bold')

plt.tight_layout()
plt.savefig('model_performance_evaluation.png', dpi=300, bbox_inches='tight')
plt.show()

print("‚úÖ Visualizations saved as 'model_performance_evaluation.png'")

# ============================================
# 4. FINAL COMPARISON TABLE
# ============================================

print("\n" + "=" * 60)
print("FINAL MODEL COMPARISON SUMMARY")
print("=" * 60)

# Create a clean final comparison table
summary_df = pd.DataFrame({
    'Model': models,
    'Test R¬≤': [f"{nn_metrics['Test']['R2']:.4f}", 
                f"{dt_metrics['Test']['R2']:.4f}", 
                f"{rf_metrics['Test']['R2']:.4f}"],
    'Test MAE': [f"${nn_metrics['Test']['MAE']:,.0f}", 
                 f"${dt_metrics['Test']['MAE']:,.0f}", 
                 f"${rf_metrics['Test']['MAE']:,.0f}"],
    'Test RMSE': [f"${nn_metrics['Test']['RMSE']:,.0f}", 
                  f"${dt_metrics['Test']['RMSE']:,.0f}", 
                  f"${rf_metrics['Test']['RMSE']:,.0f}"],
    'Train-Val Gap': [f"{nn_metrics['Training']['R2'] - nn_metrics['Validation']['R2']:.4f}", 
                      f"{dt_metrics['Training']['R2'] - dt_metrics['Validation']['R2']:.4f}", 
                      f"{rf_metrics['Training']['R2'] - rf_metrics['Validation']['R2']:.4f}"],
    'Rank (R¬≤)': ['2nd', '3rd', '1st']
})

print("\nüìä FINAL COMPARISON TABLE:")
print("-" * 80)
print(summary_df.to_string(index=False))
print("-" * 80)

print("\nüéØ BEST PERFORMING MODEL: Random Forest")
print(f"   ‚Ä¢ Test R¬≤: {rf_metrics['Test']['R2']:.4f}")
print(f"   ‚Ä¢ Test MAE: ${rf_metrics['Test']['MAE']:,.0f}")
print(f"   ‚Ä¢ Generalization Gap: {rf_metrics['Training']['R2'] - rf_metrics['Validation']['R2']:.4f} (smallest)")

print("\n" + "=" * 60)
print("‚úÖ MODEL EVALUATION COMPLETED")
print("=" * 60)
