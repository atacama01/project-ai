# ============================================
# DECISION TREE REGRESSOR IMPLEMENTATION
# ============================================

# Step 1: Import Libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.tree import DecisionTreeRegressor, plot_tree
from sklearn.model_selection import GridSearchCV, cross_val_score
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import joblib
import warnings
warnings.filterwarnings('ignore')

# Set style
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

print("=" * 60)
print("DECISION TREE REGRESSOR")
print("=" * 60)
print()

# Step 2: Load and Prepare Fresh Data
print("Loading and preprocessing data...")
try:
    # Load original dataset
    df = pd.read_csv('/kaggle/input/insurance/insurance.csv')
    print(f"âœ… Data loaded: {df.shape[0]} rows, {df.shape[1]} columns")
except:
    print("Using local file...")
    df = pd.read_csv('insurance.csv')

print()

# Step 3: Feature Engineering for Decision Tree
print("=" * 60)
print("FEATURE ENGINEERING")
print("=" * 60)

# Basic feature engineering
df['age_squared'] = df['age'] ** 2
df['bmi_squared'] = df['bmi'] ** 2
df['age_times_bmi'] = df['age'] * df['bmi'] / 100
df['smoker_age'] = df['age'] * (df['smoker'] == 'yes').astype(int)
df['smoker_bmi'] = df['bmi'] * (df['smoker'] == 'yes').astype(int)

# BMI categories
df['is_obese'] = (df['bmi'] >= 30).astype(int)
df['is_overweight'] = ((df['bmi'] >= 25) & (df['bmi'] < 30)).astype(int)

print(f"Total features created: {df.shape[1] - 1}")
print()

# Step 4: Prepare Features and Target
print("=" * 60)
print("DATA PREPARATION")
print("=" * 60)

# Separate features and target
X = df.drop(columns=['charges'])
y = df['charges']

# One-hot encode categorical variables
categorical_cols = ['sex', 'smoker', 'region']
X_encoded = pd.get_dummies(X, columns=categorical_cols, drop_first=True)
feature_names = X_encoded.columns.tolist()

print(f"Features after encoding: {len(feature_names)}")
print()

# Step 5: Train/Validation/Test Split (60/20/20)
print("=" * 60)
print("DATASET SPLITTING (60/20/20)")
print("=" * 60)

from sklearn.model_selection import train_test_split

# Split into train (60%), validation (20%), test (20%)
X_train, X_temp, y_train, y_temp = train_test_split(
    X_encoded, y, test_size=0.4, random_state=42
)

X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.5, random_state=42
)

print(f"Training set:   {X_train.shape[0]} samples")
print(f"Validation set: {X_val.shape[0]} samples")
print(f"Test set:       {X_test.shape[0]} samples")
print()

# Step 6: Baseline Decision Tree
print("=" * 60)
print("BASELINE DECISION TREE")
print("=" * 60)

baseline_dt = DecisionTreeRegressor(random_state=42)
baseline_dt.fit(X_train, y_train)

# Predictions
y_train_pred_base = baseline_dt.predict(X_train)
y_val_pred_base = baseline_dt.predict(X_val)
y_test_pred_base = baseline_dt.predict(X_test)

# Calculate metrics
def print_metrics(y_true, y_pred, set_name):
    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    r2 = r2_score(y_true, y_pred)
    print(f"{set_name:12} MAE: ${mae:,.2f} | RMSE: ${rmse:,.2f} | RÂ²: {r2:.4f}")
    return mae, rmse, r2

print("\nBaseline Performance:")
print("-" * 55)
train_mae_base, train_rmse_base, train_r2_base = print_metrics(y_train, y_train_pred_base, "Training")
val_mae_base, val_rmse_base, val_r2_base = print_metrics(y_val, y_val_pred_base, "Validation")
test_mae_base, test_rmse_base, test_r2_base = print_metrics(y_test, y_test_pred_base, "Test")
print("-" * 55)
print()

# Step 7: Hyperparameter Tuning
print("=" * 60)
print("HYPERPARAMETER OPTIMIZATION")
print("=" * 60)

# Parameter grid for tuning
param_grid = {
    'max_depth': [5, 10, 15, 20, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2', None]
}

print("Performing Grid Search Cross-Validation...")
grid_search = GridSearchCV(
    DecisionTreeRegressor(random_state=42),
    param_grid,
    cv=5,
    scoring='r2',
    n_jobs=-1,
    verbose=0
)

grid_search.fit(X_train, y_train)

print(f"âœ… Optimization completed")
print(f"Best parameters: {grid_search.best_params_}")
print()

# Step 8: Optimized Decision Tree
print("=" * 60)
print("OPTIMIZED DECISION TREE")
print("=" * 60)

best_dt = grid_search.best_estimator_
best_dt.fit(X_train, y_train)

# Predictions
y_train_pred_opt = best_dt.predict(X_train)
y_val_pred_opt = best_dt.predict(X_val)
y_test_pred_opt = best_dt.predict(X_test)

print("Optimized Performance:")
print("-" * 55)
train_mae_opt, train_rmse_opt, train_r2_opt = print_metrics(y_train, y_train_pred_opt, "Training")
val_mae_opt, val_rmse_opt, val_r2_opt = print_metrics(y_val, y_val_pred_opt, "Validation")
test_mae_opt, test_rmse_opt, test_r2_opt = print_metrics(y_test, y_test_pred_opt, "Test")
print("-" * 55)
print()

# Step 9: Feature Importance Analysis
print("=" * 60)
print("FEATURE IMPORTANCE ANALYSIS")
print("=" * 60)

# Get feature importance
importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': best_dt.feature_importances_
}).sort_values('Importance', ascending=False)

print("Top 10 Most Important Features:")
print("-" * 45)
for i, row in importance_df.head(10).iterrows():
    print(f"{row['Feature']:25} {row['Importance']:.4f}")

# Visualize
plt.figure(figsize=(10, 6))
top_features = importance_df.head(15)
colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(top_features)))

bars = plt.barh(top_features['Feature'], top_features['Importance'], color=colors)
plt.xlabel('Importance Score')
plt.title('Decision Tree Feature Importance')
plt.gca().invert_yaxis()

for bar in bars:
    width = bar.get_width()
    plt.text(width + 0.001, bar.get_y() + bar.get_height()/2,
             f'{width:.3f}', ha='left', va='center')

plt.tight_layout()
plt.savefig('decision_tree_feature_importance.png', dpi=300, bbox_inches='tight')
plt.show()
print()

# Step 10: Decision Tree Visualization
print("=" * 60)
print("DECISION TREE STRUCTURE VISUALIZATION")
print("=" * 60)

# Create simplified tree for visualization
simple_dt = DecisionTreeRegressor(max_depth=3, random_state=42)
simple_dt.fit(X_train, y_train)

plt.figure(figsize=(18, 10))
plot_tree(simple_dt, 
          feature_names=feature_names,
          filled=True, 
          rounded=True,
          fontsize=9)
plt.title("Decision Tree Structure (Depth = 3)", fontsize=16, pad=20)
plt.tight_layout()
plt.savefig('decision_tree_structure.png', dpi=300, bbox_inches='tight')
plt.show()

print("âœ… Tree visualization generated")
print()

# Step 11: Cross-Validation Analysis
print("=" * 60)
print("CROSS-VALIDATION PERFORMANCE")
print("=" * 60)

cv_scores = cross_val_score(best_dt, X_train, y_train, 
                           cv=5, scoring='r2', n_jobs=-1)

print(f"5-Fold Cross-Validation RÂ² scores: {cv_scores.round(4)}")
print(f"Mean CV RÂ²: {cv_scores.mean():.4f}")
print(f"Standard Deviation: {cv_scores.std():.4f}")
print()

# Step 12: Model Complexity Analysis
print("=" * 60)
print("MODEL COMPLEXITY ANALYSIS")
print("=" * 60)

# Analyze effect of tree depth
max_depths = range(1, 21)
train_scores = []
val_scores = []

for depth in max_depths:
    dt_temp = DecisionTreeRegressor(max_depth=depth, random_state=42)
    dt_temp.fit(X_train, y_train)
    train_scores.append(dt_temp.score(X_train, y_train))
    val_scores.append(dt_temp.score(X_val, y_val))

optimal_depth = max_depths[np.argmax(val_scores)]

plt.figure(figsize=(10, 6))
plt.plot(max_depths, train_scores, 'o-', linewidth=2, label='Training RÂ²')
plt.plot(max_depths, val_scores, 's-', linewidth=2, label='Validation RÂ²')
plt.axvline(x=optimal_depth, color='green', linestyle='--', 
            label=f'Optimal Depth ({optimal_depth})', linewidth=2)
plt.xlabel('Tree Depth')
plt.ylabel('RÂ² Score')
plt.title('Model Complexity vs Performance')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('decision_tree_complexity.png', dpi=300, bbox_inches='tight')
plt.show()

print(f"Optimal tree depth: {optimal_depth}")
print()

# Step 13: Error Analysis
print("=" * 60)
print("ERROR ANALYSIS")
print("=" * 60)

test_errors = y_test_pred_opt - y_test

plt.figure(figsize=(12, 5))

# Error distribution
plt.subplot(1, 2, 1)
plt.hist(test_errors, bins=30, color='skyblue', edgecolor='black', alpha=0.7)
plt.axvline(x=0, color='red', linestyle='--', linewidth=2)
plt.xlabel('Prediction Error ($)')
plt.ylabel('Frequency')
plt.title('Error Distribution')
plt.grid(True, alpha=0.3)

# Error vs actual values
plt.subplot(1, 2, 2)
plt.scatter(y_test, test_errors, alpha=0.5, color='orange', s=20)
plt.axhline(y=0, color='red', linestyle='--', linewidth=2)
plt.xlabel('Actual Charges ($)')
plt.ylabel('Prediction Error ($)')
plt.title('Error vs Actual Value')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('decision_tree_error_analysis.png', dpi=300, bbox_inches='tight')
plt.show()

print(f"Error Statistics:")
print(f"  Mean Error: ${test_errors.mean():,.2f}")
print(f"  Std Deviation: ${test_errors.std():,.2f}")
print(f"  Range: ${test_errors.min():,.2f} to ${test_errors.max():,.2f}")
print()

# Step 14: Prediction Visualization
print("=" * 60)
print("PREDICTION VISUALIZATION")
print("=" * 60)

fig, axes = plt.subplots(1, 3, figsize=(15, 5))

# Training set
axes[0].scatter(y_train, y_train_pred_opt, alpha=0.5, color='blue', s=15)
max_val = max(y_train.max(), y_train_pred_opt.max())
axes[0].plot([0, max_val], [0, max_val], 'r--', linewidth=2)
axes[0].set_xlabel('Actual Charges ($)')
axes[0].set_ylabel('Predicted Charges ($)')
axes[0].set_title(f'Training Set\nRÂ² = {train_r2_opt:.4f}')
axes[0].grid(True, alpha=0.3)

# Validation set
axes[1].scatter(y_val, y_val_pred_opt, alpha=0.5, color='orange', s=15)
max_val = max(y_val.max(), y_val_pred_opt.max())
axes[1].plot([0, max_val], [0, max_val], 'r--', linewidth=2)
axes[1].set_xlabel('Actual Charges ($)')
axes[1].set_ylabel('Predicted Charges ($)')
axes[1].set_title(f'Validation Set\nRÂ² = {val_r2_opt:.4f}')
axes[1].grid(True, alpha=0.3)

# Test set
axes[2].scatter(y_test, y_test_pred_opt, alpha=0.5, color='green', s=15)
max_val = max(y_test.max(), y_test_pred_opt.max())
axes[2].plot([0, max_val], [0, max_val], 'r--', linewidth=2)
axes[2].set_xlabel('Actual Charges ($)')
axes[2].set_ylabel('Predicted Charges ($)')
axes[2].set_title(f'Test Set\nRÂ² = {test_r2_opt:.4f}')
axes[2].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('decision_tree_predictions.png', dpi=300, bbox_inches='tight')
plt.show()
print()

# Step 15: Save Results
print("=" * 60)
print("SAVING MODEL AND RESULTS")
print("=" * 60)

# Save model
joblib.dump(best_dt, 'decision_tree_model.pkl')
print("âœ… Model saved as 'decision_tree_model.pkl'")

# Save predictions
predictions_df = pd.DataFrame({
    'actual': y_test,
    'predicted': y_test_pred_opt,
    'error': test_errors
})
predictions_df.to_csv('decision_tree_predictions.csv', index=False)
print("âœ… Predictions saved as 'decision_tree_predictions.csv'")

# Save feature importance
importance_df.to_csv('decision_tree_feature_importance.csv', index=False)
print("âœ… Feature importance saved as 'decision_tree_feature_importance.csv'")

# Save metrics
metrics_df = pd.DataFrame({
    'metric': ['train_r2', 'val_r2', 'test_r2', 'test_mae', 'test_rmse', 'cv_mean', 'cv_std'],
    'value': [train_r2_opt, val_r2_opt, test_r2_opt, test_mae_opt, test_rmse_opt, 
              cv_scores.mean(), cv_scores.std()]
})
metrics_df.to_csv('decision_tree_metrics.csv', index=False)
print("âœ… Metrics saved as 'decision_tree_metrics.csv'")
print()

# Step 16: Final Summary
print("=" * 60)
print("DECISION TREE - IMPLEMENTATION COMPLETE")
print("=" * 60)

print("âœ… IMPLEMENTATION STEPS COMPLETED:")
print("   1. Data loading and preprocessing")
print("   2. Feature engineering")
print("   3. Train/validation/test splitting (60/20/20)")
print("   4. Baseline model training")
print("   5. Hyperparameter optimization (Grid Search CV)")
print("   6. Optimized model training")
print("   7. Feature importance analysis")
print("   8. Model evaluation and visualization")
print("   9. Cross-validation performance")
print("  10. Results saving")

print(f"\nðŸ“Š FINAL MODEL PERFORMANCE:")
print(f"   Test RÂ² Score: {test_r2_opt:.4f}")
print(f"   Test MAE: ${test_mae_opt:,.2f}")
print(f"   Test RMSE: ${test_rmse_opt:,.2f}")
print(f"   5-Fold CV Mean RÂ²: {cv_scores.mean():.4f}")

print(f"\nðŸ” KEY FINDINGS:")
print(f"   1. Most important feature: {importance_df.iloc[0]['Feature']}")
print(f"   2. Optimal tree depth: {grid_search.best_params_.get('max_depth', 'Not limited')}")
print(f"   3. Training vs Validation gap: {train_r2_opt - val_r2_opt:.4f}")

print(f"\nðŸ’¾ OUTPUT FILES GENERATED:")
print("   decision_tree_model.pkl - Trained model")
print("   decision_tree_predictions.csv - Test predictions")
print("   decision_tree_feature_importance.csv - Feature importance")
print("   decision_tree_feature_importance.png - Feature importance plot")
print("   decision_tree_structure.png - Tree visualization")
print("   decision_tree_complexity.png - Complexity analysis")
print("   decision_tree_error_analysis.png - Error analysis")
print("   decision_tree_predictions.png - Prediction plots")
print("   decision_tree_metrics.csv - Performance metrics")

print(f"\nðŸŽ¯ ALGORITHM CHARACTERISTICS:")
print("   â€¢ Interpretable: Decision rules can be visualized")
print("   â€¢ Nonlinear: Captures complex relationships")
print("   â€¢ Feature importance: Clear ranking of feature importance")
print("   â€¢ No scaling needed: Handles different feature scales")
print("   â€¢ Prone to overfitting: Requires careful regularization")

print()
print("=" * 60)
