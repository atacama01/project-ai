# ============================================
# RANDOM FOREST IMPLEMENTATION
# ============================================

# Import additional libraries
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import KFold, cross_val_score
import time
import json
import joblib

print("=" * 60)
print("SUPER CHARGED RANDOM FOREST IMPLEMENTATION")
print("=" * 60)
print()

# Step 1: Prepare Data for Random Forest (no scaling needed for tree-based models)
print("Preparing data for Random Forest...")
print("-" * 60)

# Use the same preprocessed data but without target scaling for Random Forest
# Random Forests don't require feature scaling, but we'll use the same preprocessed features

# For Random Forest, we'll use the original target (not scaled)
# But we can try log transformation for skewed target
print("Applying log transformation to target variable (charges)...")
y_train_log = np.log1p(y_train)  # log(1 + y)
y_val_log = np.log1p(y_val)
y_test_log = np.log1p(y_test)

print(f"Original y_train mean: ${y_train.mean():,.2f}, std: ${y_train.std():,.2f}")
print(f"Log-transformed y_train mean: {y_train_log.mean():.4f}, std: {y_train_log.std():.4f}")
print()

# Step 2: Define Super Random Forest Class
print("=" * 60)
print("STEP 1: SUPER CHARGED RANDOM FOREST CONFIGURATION")
print("=" * 60)

class SuperRandomForest:
    """Super Random Forest optimized for maximum R¬≤"""
    
    def __init__(self, random_state=42, n_jobs=-1):
        
        # Three specialized Random Forests
        self.models = {
            'rf_deep': RandomForestRegressor(
                n_estimators=500,           # More trees
                max_depth=25,               # Deeper
                min_samples_split=2,        # More flexible
                min_samples_leaf=1,
                max_features='log2',        # Try log2
                max_samples=0.8,            # Subsampling
                bootstrap=True,
                random_state=random_state,
                n_jobs=n_jobs,
                oob_score=True,
                verbose=0
            ),
            'rf_robust': RandomForestRegressor(
                n_estimators=300,
                max_depth=15,               # Medium depth
                min_samples_split=10,       # More regularization
                min_samples_leaf=4,
                max_features=0.5,           # 50% features
                max_samples=0.9,
                bootstrap=True,
                random_state=random_state,
                n_jobs=n_jobs,
                oob_score=True,
                verbose=0
            ),
            'rf_fast': RandomForestRegressor(
                n_estimators=200,
                max_depth=None,             # Unlimited
                min_samples_split=5,
                min_samples_leaf=2,
                max_features='sqrt',
                bootstrap=True,
                random_state=random_state,
                n_jobs=n_jobs,
                oob_score=True,
                verbose=0
            )
        }
        
        self.weights = None
        self.is_fitted = False
        
    def fit(self, X, y):
        """Train ensemble of specialized Random Forests"""
        print("Training Super Random Forest Ensemble...")
        
        oob_scores = []
        train_times = []
        
        for name, model in self.models.items():
            print(f"\n  Training {name}...")
            start_time = time.time()
            model.fit(X, y)
            train_time = time.time() - start_time
            train_times.append(train_time)
            
            oob_score = model.oob_score_
            oob_scores.append(oob_score)
            
            print(f"    ‚úì Time: {train_time:.2f}s, OOB Score: {oob_score:.4f}")
        
        # Calculate dynamic weights based on OOB scores
        self.weights = np.array(oob_scores) / sum(oob_scores)
        
        self.is_fitted = True
        print(f"\n‚úÖ Ensemble weights: {dict(zip(self.models.keys(), self.weights.round(3)))}")
        print(f"   Weighted OOB Score: {np.average(oob_scores, weights=self.weights):.4f}")
        
    def predict(self, X, return_log=False):
        """Weighted ensemble prediction"""
        if not self.is_fitted:
            raise ValueError("Model must be fitted before prediction")
        
        predictions = np.zeros((X.shape[0], len(self.models)))
        
        for i, (name, model) in enumerate(self.models.items()):
            predictions[:, i] = model.predict(X)
        
        # Weighted average
        if return_log:
            return np.average(predictions, axis=1, weights=self.weights)
        else:
            # Convert from log to original scale
            return np.expm1(np.average(predictions, axis=1, weights=self.weights))
    
    def predict_individual(self, X, model_name, return_log=False):
        """Get predictions from individual model"""
        pred = self.models[model_name].predict(X)
        return pred if return_log else np.expm1(pred)
    
    def get_feature_importance(self):
        """Get weighted feature importance from ensemble"""
        importance_sum = np.zeros(len(self.models['rf_deep'].feature_importances_))
        
        for (name, model), weight in zip(self.models.items(), self.weights):
            importance_sum += model.feature_importances_ * weight
        
        return importance_sum / len(self.models)

# Step 3: Advanced Hyperparameter Optimization
print("\n" + "=" * 60)
print("STEP 2: ADVANCED HYPERPARAMETER OPTIMIZATION")
print("=" * 60)

print("Running targeted parameter optimization...")

# Manual parameter search (faster and more controlled)
param_combinations = []

# Generate diverse parameter combinations
for n_est in [400, 500, 600]:  # More trees
    for max_dep in [20, 25, 30, None]:  # Deeper trees
        for min_split in [2, 5, 10]:
            for min_leaf in [1, 2, 4]:
                for max_feat in ['log2', 0.3, 0.5]:
                    for max_samp in [0.7, 0.8, 0.9]:
                        param_combinations.append({
                            'n_estimators': n_est,
                            'max_depth': max_dep,
                            'min_samples_split': min_split,
                            'min_samples_leaf': min_leaf,
                            'max_features': max_feat,
                            'max_samples': max_samp,
                            'bootstrap': True,
                            'oob_score': True
                        })

print(f"Testing {len(param_combinations)} parameter combinations...")

best_score = -np.inf
best_params = None
all_results = []

# Quick evaluation of top combinations
for i, params in enumerate(param_combinations[:30]):  # Test only top 30
    model = RandomForestRegressor(
        n_estimators=params['n_estimators'],
        max_depth=params['max_depth'],
        min_samples_split=params['min_samples_split'],
        min_samples_leaf=params['min_samples_leaf'],
        max_features=params['max_features'],
        max_samples=params['max_samples'],
        bootstrap=params['bootstrap'],
        oob_score=params['oob_score'],
        random_state=42,
        n_jobs=-1,
        verbose=0
    )
    
    # Quick 3-fold CV
    kfold = KFold(n_splits=3, shuffle=True, random_state=42)
    cv_scores = cross_val_score(
        model, X_train_array, y_train_log,
        cv=kfold,
        scoring='r2',
        n_jobs=-1,
        verbose=0
    )
    
    mean_score = cv_scores.mean()
    all_results.append((params, mean_score))
    
    if mean_score > best_score:
        best_score = mean_score
        best_params = params
        print(f"  [{i+1:2d}] New best: R¬≤ = {mean_score:.4f}")

print(f"\n‚úÖ Best parameters found:")
for param, value in best_params.items():
    print(f"  {param:20}: {value}")
print(f"Best CV R¬≤: {best_score:.4f}")

# Step 4: Train Super Random Forest
print("\n" + "=" * 60)
print("STEP 3: TRAINING SUPER RANDOM FOREST")
print("=" * 60)

# Initialize and train the super ensemble
super_rf = SuperRandomForest(random_state=42, n_jobs=-1)

print("Training on log-transformed target...")
start_time = time.time()
super_rf.fit(X_train_array, y_train_log)
training_time = time.time() - start_time

print(f"\n‚úÖ Training completed in {training_time:.2f} seconds")

# Also train the best individual model
print(f"\nTraining best individual model...")
best_individual = RandomForestRegressor(
    n_estimators=best_params['n_estimators'],
    max_depth=best_params['max_depth'],
    min_samples_split=best_params['min_samples_split'],
    min_samples_leaf=best_params['min_samples_leaf'],
    max_features=best_params['max_features'],
    max_samples=best_params['max_samples'],
    bootstrap=best_params['bootstrap'],
    oob_score=best_params['oob_score'],
    random_state=42,
    n_jobs=-1,
    verbose=0
)

best_individual.fit(X_train_array, y_train_log)
print(f"‚úÖ Individual model trained, OOB Score: {best_individual.oob_score_:.4f}")

# Step 5: Comprehensive Evaluation
print("\n" + "=" * 60)
print("STEP 4: COMPREHENSIVE EVALUATION")
print("=" * 60)

# Get predictions from all models
predictions = {
    'Super Ensemble': super_rf.predict(X_val_array),
    'Best Individual': np.expm1(best_individual.predict(X_val_array)),
    'Deep RF': super_rf.predict_individual(X_val_array, 'rf_deep'),
    'Robust RF': super_rf.predict_individual(X_val_array, 'rf_robust'),
    'Fast RF': super_rf.predict_individual(X_val_array, 'rf_fast')
}

def calculate_metrics(y_true, y_pred, model_name):
    """Calculate comprehensive metrics"""
    metrics = {
        'model': model_name,
        'MAE': mean_absolute_error(y_true, y_pred),
        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),
        'R2': r2_score(y_true, y_pred),
        'MAPE': np.mean(np.abs((y_true - y_pred) / np.maximum(np.abs(y_true), 1))) * 100
    }
    return metrics

# Calculate metrics for all models
all_metrics = []
for model_name, y_pred in predictions.items():
    metrics = calculate_metrics(y_val, y_pred, model_name)
    all_metrics.append(metrics)

metrics_df = pd.DataFrame(all_metrics)

print("\nüìä MODEL PERFORMANCE COMPARISON (Validation Set):")
print("=" * 90)
print(f"{'Model':<20} {'MAE ($)':<12} {'RMSE ($)':<12} {'R¬≤':<12} {'MAPE (%)':<12}")
print("-" * 90)
for _, row in metrics_df.iterrows():
    print(f"{row['model']:<20} ${row['MAE']:<11,.0f} ${row['RMSE']:<11,.0f} "
          f"{row['R2']:<12.4f} {row['MAPE']:<12.1f}")

# Test set evaluation
print(f"\n\nüß™ TEST SET EVALUATION (Super Ensemble):")
print("-" * 60)
y_test_pred = super_rf.predict(X_test_array)
test_metrics = calculate_metrics(y_test, y_test_pred, 'Super Ensemble (Test)')

print(f"MAE:  ${test_metrics['MAE']:,.0f}")
print(f"RMSE: ${test_metrics['RMSE']:,.0f}")
print(f"R¬≤:   {test_metrics['R2']:.4f} ({test_metrics['R2']*100:.1f}% variance explained)")
print(f"MAPE: {test_metrics['MAPE']:.1f}%")

# Step 6: Feature Importance Analysis
print("\n" + "=" * 60)
print("STEP 5: FEATURE IMPORTANCE ANALYSIS")
print("=" * 60)

# Get feature importance from ensemble
feature_importance = super_rf.get_feature_importance()

importance_df = pd.DataFrame({
    'Feature': all_feature_names,  # Using your feature names from preprocessing
    'Importance': feature_importance,
    'Percent': (feature_importance / feature_importance.sum()) * 100
}).sort_values('Importance', ascending=False)

print("\nüìä TOP 15 FEATURES BY IMPORTANCE:")
print("=" * 60)
print(f"{'Rank':<6} {'Feature':<25} {'Importance':<12} {'Percent':<10}")
print("-" * 60)
for i, (_, row) in enumerate(importance_df.head(15).iterrows(), 1):
    bar = '‚ñà' * int(row['Percent'] * 2)  # Scale for visibility
    print(f"{i:<6} {row['Feature']:<25} {row['Importance']:<12.4f} {row['Percent']:<9.2f}% {bar}")

# Step 7: Key Visualizations
print("\n" + "=" * 60)
print("STEP 6: KEY VISUALIZATIONS")
print("=" * 60)

fig, axes = plt.subplots(2, 3, figsize=(18, 12))
fig.suptitle('Random Forest Analysis - Insurance Cost Prediction', fontsize=16, fontweight='bold')

# 1. Actual vs Predicted
ax1 = axes[0, 0]
ax1.scatter(y_val, predictions['Super Ensemble'], alpha=0.6, s=40, c='blue', edgecolors='black', linewidth=0.5)
max_val = max(y_val.max(), predictions['Super Ensemble'].max())
min_val = min(y_val.min(), predictions['Super Ensemble'].min())
ax1.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')
ax1.set_xlabel('Actual Charges ($)', fontweight='bold')
ax1.set_ylabel('Predicted Charges ($)', fontweight='bold')
ax1.set_title(f'Actual vs Predicted (R¬≤ = {metrics_df.loc[0, "R2"]:.4f})', fontweight='bold')
ax1.legend()
ax1.grid(True, alpha=0.3)

# 2. Feature Importance
ax2 = axes[0, 1]
top_features = importance_df.head(10)
colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(top_features)))
bars = ax2.barh(range(len(top_features)), top_features['Percent'][::-1], color=colors[::-1])
ax2.set_yticks(range(len(top_features)))
ax2.set_yticklabels(top_features['Feature'][::-1])
ax2.set_xlabel('Importance (%)', fontweight='bold')
ax2.set_title('Top 10 Feature Importance', fontweight='bold')
ax2.grid(True, alpha=0.3, axis='x')
for i, (bar, percent) in enumerate(zip(bars, top_features['Percent'][::-1])):
    ax2.text(percent + 0.5, bar.get_y() + bar.get_height()/2, 
             f'{percent:.1f}%', va='center', fontsize=9, fontweight='bold')

# 3. Model Comparison
ax3 = axes[0, 2]
models = metrics_df['model'][:3]  # Top 3 models
r2_values = metrics_df['R2'][:3]
colors = ['#2ecc71', '#3498db', '#e74c3c']
bars = ax3.bar(models, r2_values, color=colors, edgecolor='black', linewidth=1.5)
ax3.set_ylabel('R¬≤ Score', fontweight='bold')
ax3.set_title('Model Comparison (R¬≤)', fontweight='bold')
ax3.set_ylim([0.8, 0.92])
ax3.axhline(y=0.85, color='gray', linestyle=':', alpha=0.5, label='Good (0.85)')
ax3.axhline(y=0.88, color='green', linestyle='--', alpha=0.7, label='Excellent (0.88)')
ax3.legend()
ax3.grid(True, alpha=0.3, axis='y')
for bar, val in zip(bars, r2_values):
    height = bar.get_height()
    ax3.text(bar.get_x() + bar.get_width()/2, height + 0.005,
            f'{val:.4f}', ha='center', va='bottom', fontsize=10, fontweight='bold')

# 4. Residuals Plot
ax4 = axes[1, 0]
residuals = y_val - predictions['Super Ensemble']
ax4.scatter(predictions['Super Ensemble'], residuals, alpha=0.5, s=40, c='green', edgecolors='black', linewidth=0.5)
ax4.axhline(y=0, color='red', linestyle='--', linewidth=2, label='Zero Error')
ax4.set_xlabel('Predicted Charges ($)', fontweight='bold')
ax4.set_ylabel('Residuals ($)', fontweight='bold')
ax4.set_title('Residual Plot', fontweight='bold')
ax4.legend()
ax4.grid(True, alpha=0.3)

# 5. Error Distribution
ax5 = axes[1, 1]
n, bins, patches = ax5.hist(residuals, bins=35, edgecolor='black', alpha=0.7, color='coral', density=True)
ax5.axvline(x=residuals.mean(), color='red', linestyle='--', linewidth=2, 
            label=f'Mean: ${residuals.mean():,.0f}')
ax5.axvline(x=0, color='green', linestyle='-', linewidth=2, label='Zero Error')
ax5.set_xlabel('Prediction Error ($)', fontweight='bold')
ax5.set_ylabel('Density', fontweight='bold')
ax5.set_title('Error Distribution', fontweight='bold')
ax5.legend()
ax5.grid(True, alpha=0.3, axis='y')

# 6. Feature Importance Cumulative
ax6 = axes[1, 2]
cumsum = np.cumsum(importance_df['Percent'].values)
ax6.plot(range(1, len(cumsum) + 1), cumsum, marker='o', linewidth=2, 
         markersize=6, color='purple', label='Cumulative')
ax6.fill_between(range(1, len(cumsum) + 1), 0, cumsum, alpha=0.3, color='purple')

# Add threshold lines
for threshold in [80, 90, 95]:
    n_feat = np.argmax(cumsum >= threshold) + 1
    ax6.axhline(y=threshold, color='red', linestyle='--', alpha=0.5, linewidth=1)
    ax6.axvline(x=n_feat, color='red', linestyle='--', alpha=0.5, linewidth=1)
    ax6.text(n_feat + 0.5, threshold + 1, f'{threshold}%', fontsize=9, color='red', fontweight='bold')

ax6.set_xlabel('Number of Features', fontweight='bold')
ax6.set_ylabel('Cumulative Importance (%)', fontweight='bold')
ax6.set_title('Cumulative Feature Importance', fontweight='bold')
ax6.set_ylim([0, 105])
ax6.legend()
ax6.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('super_rf_analysis.png', dpi=300, bbox_inches='tight')
plt.show()

print("‚úÖ Visualizations saved!")

# Step 8: Save Results
print("\n" + "=" * 60)
print("STEP 7: SAVING RESULTS")
print("=" * 60)

timestamp = time.strftime("%Y%m%d_%H%M%S")

# Save models
model_filename = f'super_rf_ensemble_{timestamp}.joblib'
joblib.dump(super_rf, model_filename)
print(f"‚úÖ Super RF saved: {model_filename}")

# Save results
results_summary = {
    'model_info': {
        'type': 'SuperRandomForest',
        'components': list(super_rf.models.keys()),
        'weights': dict(zip(super_rf.models.keys(), super_rf.weights.tolist())),
        'training_time': round(training_time, 2),
        'timestamp': timestamp
    },
    'performance': {
        'validation': metrics_df.to_dict('records'),
        'test': test_metrics
    },
    'feature_importance': {
        'top_10': importance_df.head(10).to_dict('records')
    },
    'target_info': {
        'transformation': 'log1p',
        'original_mean': float(y_train.mean()),
        'original_std': float(y_train.std()),
        'log_mean': float(y_train_log.mean()),
        'log_std': float(y_train_log.std())
    }
}

results_filename = f'super_rf_results_{timestamp}.json'
with open(results_filename, 'w') as f:
    json.dump(results_summary, f, indent=2)
print(f"‚úÖ Results saved: {results_filename}")

# Step 9: Final Summary
print("\n" + "=" * 80)
print("‚úÖ RANDOM FOREST IMPLEMENTATION COMPLETED!")
print("=" * 80)

print(f"\nüéØ FINAL RESULTS SUMMARY:")
print("-" * 60)
print(f"Validation Performance (Super Ensemble):")
print(f"  ‚Ä¢ R¬≤:   {metrics_df.loc[0, 'R2']:.4f} ({metrics_df.loc[0, 'R2']*100:.1f}% variance explained)")
print(f"  ‚Ä¢ MAE:  ${metrics_df.loc[0, 'MAE']:,.0f}")
print(f"  ‚Ä¢ RMSE: ${metrics_df.loc[0, 'RMSE']:,.0f}")
print(f"  ‚Ä¢ MAPE: {metrics_df.loc[0, 'MAPE']:.1f}%")

print(f"\nTest Performance (Super Ensemble):")
print(f"  ‚Ä¢ R¬≤:   {test_metrics['R2']:.4f} ({test_metrics['R2']*100:.1f}% variance explained)")
print(f"  ‚Ä¢ MAE:  ${test_metrics['MAE']:,.0f}")

print(f"\nüìä PERFORMANCE CLASSIFICATION:")
print("-" * 60)
validation_r2 = metrics_df.loc[0, 'R2']
if validation_r2 >= 0.88:
    print("üèÜ EXCELLENT: R¬≤ ‚â• 0.88 (Top-tier performance)")
    print("   ‚Ä¢ Model explains ‚â•88% of variance")
elif validation_r2 >= 0.85:
    print("üëç GOOD: R¬≤ ‚â• 0.85 (Strong performance)")
    print("   ‚Ä¢ Model explains ‚â•85% of variance")
else:
    print("‚ö† MODERATE: R¬≤ < 0.85 (Needs improvement)")

print(f"\nüîë KEY SUCCESS FACTORS:")
print("-" * 60)
print("1. Super Ensemble Architecture:")
print("   ‚Ä¢ Multiple specialized Random Forests")
print("   ‚Ä¢ Weighted ensemble predictions")
print("   ‚Ä¢ Deeper trees (up to 25 depth)")
print("   ‚Ä¢ More trees (400-600 per model)")

print("\n2. Target Transformation:")
print("   ‚Ä¢ Log transformation for skewed target")
print("   ‚Ä¢ Better normal distribution for training")

print(f"\nüìà COMPARISON WITH NEURAL NETWORK:")
print("-" * 60)
# You can add your neural network results here later
print("Neural Network R¬≤: [Add your NN result here]")
print(f"Random Forest R¬≤: {test_metrics['R2']:.4f}")
print()

print("=" * 80)
